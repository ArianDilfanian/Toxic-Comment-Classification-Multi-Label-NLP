{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install Dependencies and Bring in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-gpu==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nCongratulations from me as well, use the tools well. \\xa0Â· talk \"'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(os.cpu_count())\n",
    "tf.config.threading.set_inter_op_parallelism_threads(os.cpu_count())\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join('jigsaw-toxic-comment-classification-challenge','train.csv', 'train.csv'))\n",
    "\n",
    "\n",
    "\n",
    "df.tail()\n",
    "\n",
    "\n",
    "df.iloc[5]['comment_text']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the data is translating the ssentences into our own secret language that only deep learning model understands converting each word in sentence into a unique identifire then we will create our training testing validaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- -----------\n",
      "absl-py                      2.1.0\n",
      "aiohappyeyeballs             2.4.4\n",
      "aiohttp                      3.11.11\n",
      "aiosignal                    1.3.2\n",
      "asttokens                    3.0.0\n",
      "astunparse                   1.6.3\n",
      "async-timeout                5.0.1\n",
      "attrs                        24.3.0\n",
      "blinker                      1.9.0\n",
      "Brotli                       1.1.0\n",
      "cached-property              1.5.2\n",
      "cachetools                   5.5.0\n",
      "certifi                      2024.12.14\n",
      "cffi                         1.17.1\n",
      "charset-normalizer           3.4.0\n",
      "click                        8.1.8\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.2\n",
      "contourpy                    1.3.0\n",
      "cryptography                 39.0.0\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.8.11\n",
      "decorator                    5.1.1\n",
      "exceptiongroup               1.2.2\n",
      "executing                    2.1.0\n",
      "flatbuffers                  24.3.25\n",
      "fonttools                    4.55.3\n",
      "frozenlist                   1.5.0\n",
      "gast                         0.4.0\n",
      "google-auth                  2.37.0\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.48.1\n",
      "h2                           4.1.0\n",
      "h5py                         3.9.0\n",
      "hpack                        4.0.0\n",
      "hyperframe                   6.0.1\n",
      "idna                         3.10\n",
      "importlib_metadata           8.5.0\n",
      "importlib_resources          6.4.5\n",
      "ipykernel                    6.29.5\n",
      "ipython                      8.18.1\n",
      "jedi                         0.19.2\n",
      "jupyter_client               8.6.3\n",
      "jupyter_core                 5.7.2\n",
      "keras                        2.10.0\n",
      "Keras-Preprocessing          1.1.2\n",
      "kiwisolver                   1.4.7\n",
      "libclang                     18.1.1\n",
      "Markdown                     3.6\n",
      "MarkupSafe                   3.0.2\n",
      "matplotlib                   3.9.4\n",
      "matplotlib-inline            0.1.7\n",
      "multidict                    6.1.0\n",
      "nest_asyncio                 1.6.0\n",
      "numpy                        1.26.4\n",
      "oauthlib                     3.2.2\n",
      "opencv-python                4.10.0.84\n",
      "opt_einsum                   3.4.0\n",
      "packaging                    24.2\n",
      "pandas                       2.2.3\n",
      "parso                        0.8.4\n",
      "pickleshare                  0.7.5\n",
      "pillow                       11.0.0\n",
      "pip                          24.2\n",
      "platformdirs                 4.3.6\n",
      "prompt_toolkit               3.0.48\n",
      "propcache                    0.2.1\n",
      "protobuf                     3.19.6\n",
      "psutil                       6.1.1\n",
      "pure_eval                    0.2.3\n",
      "pyasn1                       0.6.1\n",
      "pyasn1_modules               0.4.1\n",
      "pycparser                    2.22\n",
      "Pygments                     2.18.0\n",
      "PyJWT                        2.10.1\n",
      "pyOpenSSL                    23.2.0\n",
      "pyparsing                    3.2.0\n",
      "PySocks                      1.7.1\n",
      "python-dateutil              2.9.0.post0\n",
      "pytz                         2024.2\n",
      "pyu2f                        0.1.5\n",
      "pywin32                      307\n",
      "pyzmq                        26.2.0\n",
      "requests                     2.32.3\n",
      "requests-oauthlib            2.0.0\n",
      "rsa                          4.9\n",
      "scipy                        1.13.1\n",
      "setuptools                   75.1.0\n",
      "six                          1.17.0\n",
      "stack_data                   0.6.3\n",
      "tensorboard                  2.10.1\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.10.0\n",
      "tensorflow-estimator         2.10.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.5.0\n",
      "tornado                      6.4.2\n",
      "traitlets                    5.14.3\n",
      "typing_extensions            4.12.2\n",
      "tzdata                       2024.2\n",
      "urllib3                      2.3.0\n",
      "wcwidth                      0.2.13\n",
      "Werkzeug                     3.1.3\n",
      "wheel                        0.44.0\n",
      "win_inet_pton                1.1.0\n",
      "wrapt                        1.17.0\n",
      "yarl                         1.18.3\n",
      "zipp                         3.21.0\n",
      "zstandard                    0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "# TextVectorization takes a word and convert it to code for example hey might be 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Explanation\\nWhy the edits made under my usern...\n",
      "1         D'aww! He matches this background colour I'm s...\n",
      "2         Hey man, I'm really not trying to edit war. It...\n",
      "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
      "4         You, sir, are my hero. Any chance you remember...\n",
      "                                ...                        \n",
      "159566    \":::::And for the second time of asking, when ...\n",
      "159567    You should be ashamed of yourself \\n\\nThat is ...\n",
      "159568    Spitzer \\n\\nUmm, theres no actual article for ...\n",
      "159569    And it looks like it was actually you who put ...\n",
      "159570    \"\\nAnd ... I really don't think you understand...\n",
      "Name: comment_text, Length: 159571, dtype: object\n",
      "===========================================\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X = df['comment_text']\n",
    "y = df[df.columns[2:]].values\n",
    "\n",
    "print(X)\n",
    "\n",
    "print(\"===========================================\")\n",
    "\n",
    "print(y)\n",
    "\n",
    "# split our data x is our comment text y is our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 200000 # number of words in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=MAX_WORDS,\n",
    "                               output_sequence_length=1800,\n",
    "                               output_mode='int')\n",
    "\n",
    "# If MAX_WORDS=1000, only the 1000 most common tokens from the dataset will be retained.\n",
    "# All other tokens will be mapped to a special \"[UNK]\" (unknown) token.\n",
    "\n",
    "# output_sequence_length=1800 Specifies the maximum length of the output sequence.\n",
    "\n",
    "# int Converts each token into an integer corresponding to its index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer.adapt(X.values) \n",
    "\n",
    "# Purpose: This method allows the TextVectorization layer to analyze and learn the characteristics of the dataset.\n",
    "# Specifically, it builds the vocabulary based on the text data in X.values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'to',\n",
       " 'of',\n",
       " 'and',\n",
       " 'a',\n",
       " 'you',\n",
       " 'i',\n",
       " 'is',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'for',\n",
       " 'this',\n",
       " 'not',\n",
       " 'on',\n",
       " 'be',\n",
       " 'as',\n",
       " 'have',\n",
       " 'are',\n",
       " 'your',\n",
       " 'with',\n",
       " 'if',\n",
       " 'article',\n",
       " 'was',\n",
       " 'or',\n",
       " 'but',\n",
       " 'page',\n",
       " 'my',\n",
       " 'an',\n",
       " 'from',\n",
       " 'by',\n",
       " 'do',\n",
       " 'at',\n",
       " 'about',\n",
       " 'me',\n",
       " 'so',\n",
       " 'wikipedia',\n",
       " 'can',\n",
       " 'what',\n",
       " 'there',\n",
       " 'all',\n",
       " 'has',\n",
       " 'will',\n",
       " 'talk',\n",
       " 'please',\n",
       " 'would',\n",
       " 'its',\n",
       " 'no',\n",
       " 'one',\n",
       " 'just',\n",
       " 'like',\n",
       " 'they',\n",
       " 'he',\n",
       " 'dont',\n",
       " 'which',\n",
       " 'any',\n",
       " 'been',\n",
       " 'should',\n",
       " 'more',\n",
       " 'we',\n",
       " 'some',\n",
       " 'other',\n",
       " 'who',\n",
       " 'see',\n",
       " 'here',\n",
       " 'also',\n",
       " 'his',\n",
       " 'think',\n",
       " 'im',\n",
       " 'because',\n",
       " 'know',\n",
       " 'how',\n",
       " 'am',\n",
       " 'people',\n",
       " 'why',\n",
       " 'edit',\n",
       " 'articles',\n",
       " 'only',\n",
       " 'out',\n",
       " 'up',\n",
       " 'when',\n",
       " 'were',\n",
       " 'use',\n",
       " 'then',\n",
       " 'may',\n",
       " 'time',\n",
       " 'did',\n",
       " 'them',\n",
       " 'now',\n",
       " 'being',\n",
       " 'their',\n",
       " 'than',\n",
       " 'thanks',\n",
       " 'even',\n",
       " 'get',\n",
       " 'make',\n",
       " 'good',\n",
       " 'had',\n",
       " 'very',\n",
       " 'information',\n",
       " 'does',\n",
       " 'could',\n",
       " 'well',\n",
       " 'want',\n",
       " 'such',\n",
       " 'sources',\n",
       " 'way',\n",
       " 'name',\n",
       " 'these',\n",
       " 'deletion',\n",
       " 'pages',\n",
       " 'first',\n",
       " 'help',\n",
       " 'new',\n",
       " 'editing',\n",
       " 'source',\n",
       " 'go',\n",
       " 'need',\n",
       " 'say',\n",
       " 'section',\n",
       " 'edits',\n",
       " 'again',\n",
       " 'thank',\n",
       " 'where',\n",
       " 'user',\n",
       " 'made',\n",
       " 'many',\n",
       " 'much',\n",
       " 'really',\n",
       " 'used',\n",
       " 'most',\n",
       " 'discussion',\n",
       " 'find',\n",
       " 'same',\n",
       " 'ive',\n",
       " 'deleted',\n",
       " 'into',\n",
       " 'fuck',\n",
       " 'those',\n",
       " 'work',\n",
       " 'since',\n",
       " 'before',\n",
       " 'after',\n",
       " 'point',\n",
       " 'add',\n",
       " 'look',\n",
       " 'right',\n",
       " 'read',\n",
       " 'image',\n",
       " 'take',\n",
       " 'still',\n",
       " 'over',\n",
       " 'someone',\n",
       " 'him',\n",
       " 'two',\n",
       " 'back',\n",
       " 'too',\n",
       " 'fact',\n",
       " 'link',\n",
       " 'said',\n",
       " 'own',\n",
       " 'something',\n",
       " 'going',\n",
       " 'youre',\n",
       " 'blocked',\n",
       " 'list',\n",
       " 'stop',\n",
       " 'without',\n",
       " 'content',\n",
       " 'hi',\n",
       " 'under',\n",
       " 'editors',\n",
       " 'our',\n",
       " 'block',\n",
       " 'thats',\n",
       " 'us',\n",
       " 'added',\n",
       " 'utc',\n",
       " 'history',\n",
       " 'another',\n",
       " 'doesnt',\n",
       " 'removed',\n",
       " 'might',\n",
       " 'note',\n",
       " 'however',\n",
       " 'sure',\n",
       " 'place',\n",
       " 'never',\n",
       " 'done',\n",
       " 'welcome',\n",
       " 'her',\n",
       " 'case',\n",
       " 'put',\n",
       " 'personal',\n",
       " 'seems',\n",
       " 'reason',\n",
       " 'better',\n",
       " 'using',\n",
       " 'yourself',\n",
       " 'cant',\n",
       " 'actually',\n",
       " 'ask',\n",
       " 'comment',\n",
       " 'while',\n",
       " 'vandalism',\n",
       " 'feel',\n",
       " 'question',\n",
       " 'anything',\n",
       " 'believe',\n",
       " 'person',\n",
       " 'links',\n",
       " 'things',\n",
       " 'both',\n",
       " 'didnt',\n",
       " 'comments',\n",
       " 'best',\n",
       " 'ill',\n",
       " 'part',\n",
       " 'she',\n",
       " 'hope',\n",
       " 'policy',\n",
       " 'against',\n",
       " 'off',\n",
       " 'keep',\n",
       " 'already',\n",
       " 'free',\n",
       " 'wiki',\n",
       " 'thing',\n",
       " 'nothing',\n",
       " 'change',\n",
       " 'wrong',\n",
       " 'though',\n",
       " 'problem',\n",
       " 'remove',\n",
       " 'little',\n",
       " 'subject',\n",
       " 'â¢',\n",
       " 'others',\n",
       " 'trying',\n",
       " 'tag',\n",
       " 'copyright',\n",
       " 'must',\n",
       " 'understand',\n",
       " 'above',\n",
       " 'few',\n",
       " 'anyone',\n",
       " 'speedy',\n",
       " 'last',\n",
       " 'issue',\n",
       " 'give',\n",
       " 'questions',\n",
       " 'agree',\n",
       " 'rather',\n",
       " 'years',\n",
       " 'let',\n",
       " '2',\n",
       " 'different',\n",
       " 'editor',\n",
       " 'long',\n",
       " 'reliable',\n",
       " 'making',\n",
       " 'world',\n",
       " 'come',\n",
       " 'sorry',\n",
       " 'isnt',\n",
       " 'reference',\n",
       " 'mean',\n",
       " 'continue',\n",
       " 'try',\n",
       " 'references',\n",
       " 'found',\n",
       " 'doing',\n",
       " 'text',\n",
       " 'great',\n",
       " 'leave',\n",
       " 'says',\n",
       " 'got',\n",
       " 'probably',\n",
       " 'english',\n",
       " 'original',\n",
       " 'every',\n",
       " '1',\n",
       " 'simply',\n",
       " 'word',\n",
       " 'users',\n",
       " 'fair',\n",
       " 'hello',\n",
       " 'either',\n",
       " 'check',\n",
       " 'least',\n",
       " 'adding',\n",
       " 'ip',\n",
       " 'show',\n",
       " 'site',\n",
       " 'state',\n",
       " 'else',\n",
       " 'delete',\n",
       " 'consensus',\n",
       " 'enough',\n",
       " 'request',\n",
       " 'far',\n",
       " 'opinion',\n",
       " 'created',\n",
       " 'around',\n",
       " 'life',\n",
       " 'day',\n",
       " 'between',\n",
       " 'through',\n",
       " 'example',\n",
       " 'view',\n",
       " 'yes',\n",
       " 'reverted',\n",
       " 'yet',\n",
       " 'etc',\n",
       " 'id',\n",
       " 'matter',\n",
       " 'shit',\n",
       " 'u',\n",
       " 'war',\n",
       " 'notable',\n",
       " 'contributions',\n",
       " 'given',\n",
       " 'thought',\n",
       " 'material',\n",
       " 'book',\n",
       " 'admin',\n",
       " 'write',\n",
       " 'post',\n",
       " 'down',\n",
       " 'account',\n",
       " 'clearly',\n",
       " 'having',\n",
       " 'encyclopedia',\n",
       " 'lot',\n",
       " 'support',\n",
       " 'real',\n",
       " 'bad',\n",
       " 'message',\n",
       " 'needs',\n",
       " 'images',\n",
       " 'tell',\n",
       " 'seem',\n",
       " 'called',\n",
       " 'maybe',\n",
       " 'evidence',\n",
       " 'instead',\n",
       " 'ever',\n",
       " '3',\n",
       " 'correct',\n",
       " 'saying',\n",
       " 'clear',\n",
       " 'always',\n",
       " 'number',\n",
       " 'important',\n",
       " 'further',\n",
       " 'quite',\n",
       " 'perhaps',\n",
       " 'old',\n",
       " 'â',\n",
       " 'true',\n",
       " 'until',\n",
       " 'hate',\n",
       " 'states',\n",
       " 'whether',\n",
       " 'consider',\n",
       " 'written',\n",
       " 'claim',\n",
       " 'language',\n",
       " 'media',\n",
       " 'bit',\n",
       " 'once',\n",
       " 'guidelines',\n",
       " 'term',\n",
       " 'criteria',\n",
       " 'research',\n",
       " 'nigger',\n",
       " 'version',\n",
       " 'times',\n",
       " 'website',\n",
       " 'getting',\n",
       " 'fucking',\n",
       " 'theres',\n",
       " 'review',\n",
       " 'mention',\n",
       " 'pov',\n",
       " 'oh',\n",
       " 'makes',\n",
       " 'several',\n",
       " 'revert',\n",
       " 'considered',\n",
       " 'changes',\n",
       " 'cannot',\n",
       " 'words',\n",
       " 'idea',\n",
       " 'title',\n",
       " 'suck',\n",
       " 'address',\n",
       " 'notice',\n",
       " 'based',\n",
       " 'top',\n",
       " 'following',\n",
       " 'current',\n",
       " 'each',\n",
       " 'listed',\n",
       " 'means',\n",
       " 'possible',\n",
       " 'group',\n",
       " 'facts',\n",
       " 'regarding',\n",
       " 'care',\n",
       " 'rules',\n",
       " 'second',\n",
       " 'main',\n",
       " 'template',\n",
       " 'mentioned',\n",
       " 'general',\n",
       " 'year',\n",
       " 'attack',\n",
       " 'kind',\n",
       " 'whole',\n",
       " 'course',\n",
       " 'statement',\n",
       " 'left',\n",
       " 'hey',\n",
       " 'date',\n",
       " 'include',\n",
       " 'seen',\n",
       " 'three',\n",
       " 'issues',\n",
       " 'start',\n",
       " 'ass',\n",
       " 'ok',\n",
       " 'end',\n",
       " 'wikipedias',\n",
       " 'call',\n",
       " 'less',\n",
       " 'topic',\n",
       " 'gay',\n",
       " 'suggest',\n",
       " 'man',\n",
       " 'including',\n",
       " 'happy',\n",
       " 'sense',\n",
       " 'provide',\n",
       " 'create',\n",
       " 'big',\n",
       " 'days',\n",
       " 'myself',\n",
       " 'american',\n",
       " 'redirect',\n",
       " 'known',\n",
       " 'sentence',\n",
       " 'move',\n",
       " 'appropriate',\n",
       " 'changed',\n",
       " 'love',\n",
       " 'notability',\n",
       " 'explain',\n",
       " 'started',\n",
       " 'included',\n",
       " 'removing',\n",
       " 'project',\n",
       " 'anyway',\n",
       " 'info',\n",
       " 'mind',\n",
       " 'school',\n",
       " '2005',\n",
       " 'next',\n",
       " 'looking',\n",
       " 'although',\n",
       " 'picture',\n",
       " 'relevant',\n",
       " 'four',\n",
       " 'die',\n",
       " 'sign',\n",
       " 'answer',\n",
       " 'style',\n",
       " 'away',\n",
       " 'per',\n",
       " 'order',\n",
       " 'warning',\n",
       " 'wont',\n",
       " 'recent',\n",
       " 'youve',\n",
       " 'interest',\n",
       " 'community',\n",
       " 'summary',\n",
       " 'later',\n",
       " 'lol',\n",
       " 'claims',\n",
       " 'currently',\n",
       " 'discuss',\n",
       " 'interested',\n",
       " 'policies',\n",
       " 'attacks',\n",
       " 'especially',\n",
       " 'wish',\n",
       " 'wrote',\n",
       " 'able',\n",
       " 'specific',\n",
       " 'public',\n",
       " 'taken',\n",
       " 'writing',\n",
       " 'neutral',\n",
       " 'full',\n",
       " 'names',\n",
       " 'within',\n",
       " '4',\n",
       " 'position',\n",
       " 'related',\n",
       " 'below',\n",
       " 'line',\n",
       " 'wanted',\n",
       " 'during',\n",
       " 'appears',\n",
       " 'stuff',\n",
       " 'certainly',\n",
       " 'official',\n",
       " 'nice',\n",
       " 'itself',\n",
       " 'faith',\n",
       " 'everyone',\n",
       " 'wasnt',\n",
       " 'live',\n",
       " 'report',\n",
       " 'completely',\n",
       " 'according',\n",
       " 'unless',\n",
       " 'common',\n",
       " 'pretty',\n",
       " 'country',\n",
       " 'everything',\n",
       " 'looks',\n",
       " 'due',\n",
       " 'single',\n",
       " 'hes',\n",
       " 'process',\n",
       " 'contribs',\n",
       " 'news',\n",
       " 'involved',\n",
       " 'god',\n",
       " 'fat',\n",
       " 'therefore',\n",
       " 'obviously',\n",
       " 'remember',\n",
       " 'lead',\n",
       " 'hard',\n",
       " 'admins',\n",
       " 'came',\n",
       " 'edited',\n",
       " 'web',\n",
       " 'stay',\n",
       " 'learn',\n",
       " 'response',\n",
       " 'future',\n",
       " 'past',\n",
       " 'asked',\n",
       " 'truth',\n",
       " 'reading',\n",
       " 'power',\n",
       " '2006',\n",
       " 'stupid',\n",
       " 'entry',\n",
       " 'quote',\n",
       " 'posted',\n",
       " 'nor',\n",
       " 'talking',\n",
       " 'placed',\n",
       " '5',\n",
       " 'ago',\n",
       " 'similar',\n",
       " 'email',\n",
       " 'game',\n",
       " 'published',\n",
       " 'exactly',\n",
       " 'today',\n",
       " 'reasons',\n",
       " 'paragraph',\n",
       " 'faggot',\n",
       " 'city',\n",
       " 'argument',\n",
       " 'whatever',\n",
       " 'system',\n",
       " 'working',\n",
       " 'false',\n",
       " 'sandbox',\n",
       " 'moron',\n",
       " 'political',\n",
       " 'noticed',\n",
       " 'useful',\n",
       " 'havent',\n",
       " 'guy',\n",
       " 'high',\n",
       " 'regards',\n",
       " 'united',\n",
       " 'guess',\n",
       " 'appreciate',\n",
       " 'particular',\n",
       " 'deleting',\n",
       " 'form',\n",
       " 'books',\n",
       " 'government',\n",
       " 'dispute',\n",
       " 'five',\n",
       " 'british',\n",
       " 'reverting',\n",
       " 'major',\n",
       " 'problems',\n",
       " 'national',\n",
       " 'party',\n",
       " 'provided',\n",
       " 'often',\n",
       " 'ones',\n",
       " 'become',\n",
       " 'lets',\n",
       " 'tried',\n",
       " 'side',\n",
       " 'administrator',\n",
       " 'along',\n",
       " 'reply',\n",
       " 'almost',\n",
       " 'needed',\n",
       " 'stated',\n",
       " 'rule',\n",
       " 'took',\n",
       " 'search',\n",
       " 'knowledge',\n",
       " 'banned',\n",
       " 'cheers',\n",
       " 'taking',\n",
       " 'vandalize',\n",
       " 'â',\n",
       " 'certain',\n",
       " '2007',\n",
       " 'username',\n",
       " 'fine',\n",
       " 'status',\n",
       " 'law',\n",
       " 'points',\n",
       " 'company',\n",
       " 'otherwise',\n",
       " 'uploaded',\n",
       " 'terms',\n",
       " 'explanation',\n",
       " 'generally',\n",
       " 'sort',\n",
       " 'entire',\n",
       " 'shows',\n",
       " 'description',\n",
       " 'whats',\n",
       " 'recently',\n",
       " 'follow',\n",
       " 'guys',\n",
       " '2008',\n",
       " 'likely',\n",
       " 'film',\n",
       " 'present',\n",
       " 'aware',\n",
       " 'saw',\n",
       " 'definition',\n",
       " 'cited',\n",
       " 'alone',\n",
       " 'google',\n",
       " 'music',\n",
       " 'soon',\n",
       " 'indeed',\n",
       " 'decide',\n",
       " 'ban',\n",
       " 'wp',\n",
       " 'appear',\n",
       " 'views',\n",
       " 'week',\n",
       " 'open',\n",
       " 'citation',\n",
       " 'contributing',\n",
       " 'actual',\n",
       " 'set',\n",
       " 'interesting',\n",
       " 'piece',\n",
       " 'c',\n",
       " 'short',\n",
       " 'white',\n",
       " 'told',\n",
       " 'theory',\n",
       " 'area',\n",
       " 'improve',\n",
       " 'external',\n",
       " 'small',\n",
       " 'story',\n",
       " 'contact',\n",
       " 'simple',\n",
       " '2004',\n",
       " 'various',\n",
       " 'allowed',\n",
       " 'moved',\n",
       " 'test',\n",
       " 'internet',\n",
       " 'obvious',\n",
       " 'family',\n",
       " 'band',\n",
       " 'attention',\n",
       " 'arent',\n",
       " 'proposed',\n",
       " 'jew',\n",
       " 'themselves',\n",
       " 'members',\n",
       " 'wouldnt',\n",
       " 'result',\n",
       " 'disagree',\n",
       " 'thus',\n",
       " 'cunt',\n",
       " 'went',\n",
       " 'type',\n",
       " 'sites',\n",
       " 'ie',\n",
       " 'context',\n",
       " 'mr',\n",
       " 'previous',\n",
       " 'nonsense',\n",
       " 'actions',\n",
       " 'tags',\n",
       " 'cite',\n",
       " 'works',\n",
       " '10',\n",
       " 'citations',\n",
       " 'jews',\n",
       " 'university',\n",
       " 're',\n",
       " 'enjoy',\n",
       " 'conflict',\n",
       " 'hours',\n",
       " 'shouldnt',\n",
       " 'proper',\n",
       " 'bias',\n",
       " 'category',\n",
       " 'job',\n",
       " 'longer',\n",
       " 'file',\n",
       " 'together',\n",
       " 'hell',\n",
       " 'sourced',\n",
       " 'sucks',\n",
       " 'addition',\n",
       " 'happened',\n",
       " 'avoid',\n",
       " 'automatically',\n",
       " 'author',\n",
       " 'valid',\n",
       " 'black',\n",
       " 'creating',\n",
       " 'deal',\n",
       " 'worked',\n",
       " 'npov',\n",
       " 'goes',\n",
       " 'himself',\n",
       " 'seriously',\n",
       " 'john',\n",
       " 'death',\n",
       " 'proof',\n",
       " 'respect',\n",
       " 'bitch',\n",
       " 'science',\n",
       " 'human',\n",
       " 'biased',\n",
       " 'comes',\n",
       " 'helpful',\n",
       " 'large',\n",
       " 'accepted',\n",
       " 'available',\n",
       " 'exist',\n",
       " 'series',\n",
       " 'tildes',\n",
       " 'opinions',\n",
       " 'hand',\n",
       " '6',\n",
       " 'indicate',\n",
       " 'sections',\n",
       " 'rights',\n",
       " 'necessary',\n",
       " 'act',\n",
       " 'meaning',\n",
       " 'attempt',\n",
       " 'accept',\n",
       " 'personally',\n",
       " 'statements',\n",
       " 'violation',\n",
       " 'months',\n",
       " 'criticism',\n",
       " 'accurate',\n",
       " 'action',\n",
       " 'usually',\n",
       " 'unblock',\n",
       " 'german',\n",
       " 'pig',\n",
       " 'cause',\n",
       " 'yeah',\n",
       " 'living',\n",
       " 'copy',\n",
       " 'debate',\n",
       " 'upon',\n",
       " 'assume',\n",
       " 'july',\n",
       " 'calling',\n",
       " 'standard',\n",
       " 'video',\n",
       " 'play',\n",
       " 'rest',\n",
       " 'tagged',\n",
       " 'doubt',\n",
       " 'sex',\n",
       " 'multiple',\n",
       " 'theyre',\n",
       " 'historical',\n",
       " 'serious',\n",
       " 'details',\n",
       " 'dick',\n",
       " 'youll',\n",
       " 'separate',\n",
       " 'manual',\n",
       " 'record',\n",
       " 'blocking',\n",
       " 'afd',\n",
       " 'explaining',\n",
       " 'situation',\n",
       " 'refer',\n",
       " 'wikiproject',\n",
       " 'heard',\n",
       " 'online',\n",
       " 'level',\n",
       " 'fix',\n",
       " 'asking',\n",
       " '7',\n",
       " 'complete',\n",
       " 'speak',\n",
       " 'lack',\n",
       " 'messages',\n",
       " 'none',\n",
       " 'prove',\n",
       " 'third',\n",
       " 'subjects',\n",
       " 'church',\n",
       " 'apparently',\n",
       " '2009',\n",
       " 'south',\n",
       " 'rationale',\n",
       " 'bullshit',\n",
       " 'data',\n",
       " 'directly',\n",
       " 'august',\n",
       " 'period',\n",
       " 'legal',\n",
       " 'behavior',\n",
       " 'difference',\n",
       " 'contribute',\n",
       " 'greek',\n",
       " 'huge',\n",
       " 'gets',\n",
       " 'wikipedian',\n",
       " 'couple',\n",
       " 'supposed',\n",
       " 'among',\n",
       " 'early',\n",
       " 'except',\n",
       " 'march',\n",
       " 'close',\n",
       " 'quality',\n",
       " 'space',\n",
       " 'meant',\n",
       " 'countries',\n",
       " 'run',\n",
       " 'team',\n",
       " 'uses',\n",
       " 'military',\n",
       " 'b',\n",
       " 'changing',\n",
       " 'existing',\n",
       " 'specifically',\n",
       " 'significant',\n",
       " '2010',\n",
       " 'pillars',\n",
       " 'fish',\n",
       " 'incorrect',\n",
       " 'culture',\n",
       " 'described',\n",
       " 'produce',\n",
       " 'jewish',\n",
       " '24',\n",
       " 'uk',\n",
       " 'disruptive',\n",
       " 'd',\n",
       " 'field',\n",
       " 'error',\n",
       " 'india',\n",
       " 'head',\n",
       " 'primary',\n",
       " 'friend',\n",
       " 'earlier',\n",
       " 'sometimes',\n",
       " 'outside',\n",
       " '20',\n",
       " 'purpose',\n",
       " 'administrators',\n",
       " 'modern',\n",
       " 'photo',\n",
       " 'table',\n",
       " 'particularly',\n",
       " 't',\n",
       " 'release',\n",
       " 'gave',\n",
       " 'box',\n",
       " 'cases',\n",
       " 'inclusion',\n",
       " 'born',\n",
       " 'pictures',\n",
       " 'readers',\n",
       " 'june',\n",
       " 'character',\n",
       " 'vote',\n",
       " 'okay',\n",
       " 'groups',\n",
       " 'anonymous',\n",
       " 'abuse',\n",
       " 'arguments',\n",
       " 'business',\n",
       " 'shall',\n",
       " 'sock',\n",
       " 'tutorial',\n",
       " 'january',\n",
       " 'friends',\n",
       " 'numbers',\n",
       " 'control',\n",
       " 'thinking',\n",
       " 'member',\n",
       " 'linked',\n",
       " 'happen',\n",
       " 'reported',\n",
       " 'contest',\n",
       " 'coming',\n",
       " 'takes',\n",
       " 'concerns',\n",
       " 'allow',\n",
       " 'wait',\n",
       " 'majority',\n",
       " 'giving',\n",
       " '8',\n",
       " 'bring',\n",
       " 'eg',\n",
       " 'worth',\n",
       " 'kill',\n",
       " 'totally',\n",
       " 'red',\n",
       " 'force',\n",
       " 'decided',\n",
       " 'discussed',\n",
       " 'house',\n",
       " 'finally',\n",
       " 'absolutely',\n",
       " 'putting',\n",
       " 'scientific',\n",
       " 'respond',\n",
       " 'mistake',\n",
       " 'decision',\n",
       " 'de',\n",
       " 'lost',\n",
       " 'entirely',\n",
       " '100',\n",
       " 'towards',\n",
       " 'merely',\n",
       " 'home',\n",
       " 'neither',\n",
       " 'dear',\n",
       " 'independent',\n",
       " 'international',\n",
       " 'song',\n",
       " 'balls',\n",
       " 'wants',\n",
       " 'possibly',\n",
       " 'unsigned',\n",
       " 'million',\n",
       " 'irrelevant',\n",
       " 'standards',\n",
       " 'april',\n",
       " '12',\n",
       " 'press',\n",
       " 'figure',\n",
       " 'organization',\n",
       " 'looked',\n",
       " 'inappropriate',\n",
       " 'chance',\n",
       " 'posting',\n",
       " 'population',\n",
       " 'advice',\n",
       " 'posts',\n",
       " 'north',\n",
       " 'events',\n",
       " 'unfortunately',\n",
       " 'named',\n",
       " 'album',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()\n",
    "\n",
    "# Purpose: Retrieves the learned vocabulary after the adapt() method has been called.\n",
    "\n",
    "# Returns:\n",
    "# A list of tokens (words or subwords) in the vocabulary, sorted by frequency in descending order \n",
    "# (most frequent tokens appear first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Is built with CUDA: False\n",
      "GPUs available: []\n"
     ]
    }
   ],
   "source": [
    "# Test out tokenizer with random sentence\n",
    "\n",
    "vectorizer('Hello world, im arian life is good')[:5]\n",
    "\n",
    "# 'hello' â 288\n",
    "# 'world' â 263\n",
    "# 'im' â 70\n",
    "# 'arian' â 44209   Uncommon or dataset-specific token\n",
    "# 'life' â 306\n",
    "# 'is' â 2\n",
    "# 'good' â 29\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Is built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  645    76     2 ...     0     0     0]\n",
      " [    1    54  2489 ...     0     0     0]\n",
      " [  425   441    70 ...     0     0     0]\n",
      " ...\n",
      " [32445  7392   383 ...     0     0     0]\n",
      " [    5    12   534 ...     0     0     0]\n",
      " [    5     8   130 ...     0     0     0]], shape=(159571, 1800), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of our x values\n",
    "vectorized_text = vectorizer(X.values)\n",
    "\n",
    "print(vectorized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use pipeline ?\n",
    "\n",
    "\n",
    "This approach creates a highly efficient input pipeline:\n",
    "\n",
    "1- Scalability: Supports large datasets without loading all data into memory at once.\n",
    "\n",
    "2- Parallelism: Takes advantage of CPU/GPU parallel processing.\n",
    "\n",
    "3- Performance: Minimizes training bottlenecks by caching, shuffling, batching, and prefetching data ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data pipeline passing our data\n",
    "# map, chache, shuffle, batch, prefetch  from_tensor_slices, list_file\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y)) # input features and y is a target value\n",
    "\n",
    "\n",
    "dataset = dataset.cache() # Purpose: Caches the dataset in memory or a local file, depending on available resources.\n",
    "# Benefit: Speeds up the data pipeline by avoiding recomputation or reloading of data for each epoch.\n",
    "\n",
    "dataset = dataset.shuffle(10000) # Purpose: Randomizes the order of elements in the dataset to prevent training bias.\n",
    "\n",
    "dataset = dataset.batch(112) # 16 data in each batches \n",
    "# Benefit: Allows the model to process multiple data points simultaneously,\n",
    "# leveraging GPU/TPU parallelism and improving computational efficiency.\n",
    "\n",
    "\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    " # Purpose: Ensures that data loading and preprocessing are done asynchronously and ahead of time\n",
    "# while the model is training on the current batch.\n",
    "# Benefit: Reduces input pipeline bottlenecks by overlapping data loading with model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = dataset.take(int(len(dataset)*.7))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeding layer\n",
    "\n",
    "Purpose: Converting Integer-Encoded Words into Dense Vectors\n",
    "\n",
    "Why convert words into vectors?\n",
    "\n",
    "Machine learning models, particularly neural networks, work with numbers rather than words.\n",
    "\n",
    "Words are first converted into integer indices (e.g., {\"the\": 1, \"cat\": 2, \"sat\": 3}) using techniques like tokenization.\n",
    "\n",
    "However, integer indices alone donât provide meaningful representations of the relationships between words.\n",
    "\n",
    "An Embedding layer maps each integer index to a dense vector of fixed size\n",
    "\n",
    "============================================================================================\n",
    "\n",
    "\n",
    "LSTMs are designed to handle sequential data, such as text, audio, or time-series data.\n",
    "\n",
    "The Bidirectional LSTM ensures that patterns are not missed regardless of whether they appear early or late in the sequence.\n",
    "\n",
    "Why bidirectional?\n",
    "\n",
    "Text sequences often have meaning that depends on the context of surrounding words. For example, in the sentence:\n",
    "\n",
    "\"She did not like the movie.\"\n",
    "\n",
    "The word \"like\" depends on the preceding word \"not\" to convey the correct sentiment.\n",
    "\n",
    "A Bidirectional LSTM processes the sequence from:\n",
    "\n",
    "Left to right (forward): Captures dependencies in the order words naturally appear.\n",
    "\n",
    "Right to left (backward): Captures dependencies looking back at earlier words.\n",
    "\n",
    "By combining the outputs of the forward and backward LSTMs, the model understands the context from both directions, leading to better feature representations.\n",
    "\n",
    "The activation function in the LSTM cell is typically tanh for the hidden state because:\n",
    "\n",
    "tanh squashes values into the range [-1, 1], providing a smooth gradient and preventing extreme values.\n",
    "\n",
    "\n",
    "===================================================================================================\n",
    "\n",
    "Dense\n",
    "\n",
    "Purpose: These layers act as feature extractors to transform the sequence-level features into higher-dimensional spaces.\n",
    "\n",
    "activation='relu': The Rectified Linear Unit (ReLU) activation introduces non-linearity to the model.\n",
    "\n",
    "Role: These layers allow the model to learn complex patterns from the temporal features extracted by the Bidirectional LSTM.\n",
    "\n",
    "====================================================================================================\n",
    "\n",
    "Final layer\n",
    "\n",
    "Purpose: Outputs predictions for the target variable(s).\n",
    "\n",
    "\n",
    "6: Number of output units, corresponding to the 6 classes or labels.\n",
    "\n",
    "activation='sigmoid': Produces outputs in the range [0, 1], suitable for multi-label classification (where each class can independently be \"on\" or \"off\").\n",
    "\n",
    "Role: Outputs probabilities for each of the 6 labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Create the embedding layer \n",
    "model.add(Embedding(MAX_WORDS+1, 32))\n",
    "# Bidirectional LSTM Layer\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "# Feature extractor Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Final layer \n",
    "model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')\n",
    "# we have got six values coming out of the shouldnt this loss be something line categorical cross entropy ?\n",
    "# NO we are running six different binary classifires at one time 6 values that are either 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          6400032   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               16640     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,491,686\n",
      "Trainable params: 6,491,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools to Decide the Number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This ensures the model stops training when performance stops improving.\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',  # Monitor validation accuracy\n",
    "    mode='max',              # Maximize validation accuracy\n",
    "    patience=3,              # Stop if no improvement after 3 epochs\n",
    "    restore_best_weights=True  # Restore the best weights after stopping\n",
    ")\n",
    "\n",
    "history = model.fit(train, validation_data=val, epochs=10, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# history = model.fit(train, epochs=1, validation_data=val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Input text for prediction\n",
    "input_text = [\"You freaking suck! I am going to hit you.\"]\n",
    "\n",
    "# Use the vectorizer to tokenize and pad the input text\n",
    "tokenized_input = vectorizer(input_text)\n",
    "\n",
    "# Convert to a NumPy array (optional, if your model requires it)\n",
    "tokenized_input = tokenized_input.numpy()\n",
    "\n",
    "# Make the prediction\n",
    "res = model.predict(tokenized_input)\n",
    "print(\"Prediction:\", res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(res > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X, batch_y = test.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(batch_X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator(): \n",
    "    # Unpack the batch \n",
    "    X_true, y_true = batch\n",
    "    # Make a prediction \n",
    "    yhat = model.predict(X_true)\n",
    "    \n",
    "    # Flatten the predictions\n",
    "    y_true = y_true.flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    \n",
    "    pre.update_state(y_true, yhat)\n",
    "    re.update_state(y_true, yhat)\n",
    "    acc.update_state(y_true, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Precision: {pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test and Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting fastapi<1.0 (from gradio)\n",
      "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio)\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub>=0.19.3 (from gradio)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gradio) (6.4.5)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gradio) (3.9.4)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.14-cp39-cp39-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Collecting pillow<11.0,>=8.0 (from gradio)\n",
      "  Downloading pillow-10.4.0-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pyyaml<7.0,>=5.0 (from gradio)\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.9.0-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gradio) (2.3.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from gradio-client==1.3.0->gradio)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
      "  Downloading websockets-12.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1.0->gradio)\n",
      "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.19.3->gradio)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.19.3->gradio)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.21.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.4.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "   ---------------------------------------- 0.0/18.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/18.1 MB 3.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.6/18.1 MB 3.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.4/18.1 MB 3.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 3.1/18.1 MB 3.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 3.4/18.1 MB 3.0 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 4.2/18.1 MB 3.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 5.0/18.1 MB 3.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 5.2/18.1 MB 3.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.5/18.1 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 6.3/18.1 MB 2.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 6.8/18.1 MB 2.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 7.6/18.1 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 8.4/18.1 MB 3.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 8.9/18.1 MB 3.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 9.4/18.1 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.2/18.1 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.2/18.1 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.2/18.1 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 11.8/18.1 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 11.8/18.1 MB 2.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 12.3/18.1 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 13.1/18.1 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 13.4/18.1 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.2/18.1 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 14.7/18.1 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 15.2/18.1 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 15.5/18.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 15.5/18.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.0/18.1 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.3/18.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.8/18.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.1/18.1 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Downloading orjson-3.10.14-cp39-cp39-win_amd64.whl (133 kB)\n",
      "Downloading pillow-10.4.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.1/2.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.0/2.0 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.6 MB/s eta 0:00:00\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Downloading ruff-0.9.0-py3-none-win_amd64.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 3.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/9.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/9.7 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.4/9.7 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.7 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.7 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.0/9.7 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.8/9.7 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.0/9.7 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.7 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.9/9.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 2.9 MB/s eta 0:00:00\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Using cached ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading websockets-12.0-cp39-cp39-win_amd64.whl (124 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydub, websockets, tqdm, tomlkit, sniffio, shellingham, semantic-version, ruff, pyyaml, python-multipart, pydantic-core, pillow, orjson, mdurl, markupsafe, h11, fsspec, filelock, ffmpy, annotated-types, aiofiles, uvicorn, pydantic, markdown-it-py, jinja2, huggingface-hub, httpcore, anyio, starlette, rich, httpx, typer, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 anyio-4.8.0 fastapi-0.115.6 ffmpy-0.5.0 filelock-3.16.1 fsspec-2024.12.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.27.1 jinja2-3.1.5 markdown-it-py-3.0.0 markupsafe-2.1.5 mdurl-0.1.2 orjson-3.10.14 pillow-10.4.0 pydantic-2.10.5 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 pyyaml-6.0.2 rich-13.9.4 ruff-0.9.0 semantic-version-2.10.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.41.3 tomlkit-0.12.0 tqdm-4.67.1 typer-0.15.1 uvicorn-0.34.0 websockets-12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\arian\\anaconda3\\envs\\tf_gpu\\Lib\\site-packages\\~il'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arian\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('toxicity.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('toxicity.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_str = vectorizer('hey i freaken hate you!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 627ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(np.expand_dims(input_str,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49944752, 0.4986473 , 0.50177884, 0.4999523 , 0.49915576,\n",
       "        0.5007688 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "sample_input =\"you are stupid\"\n",
    "\n",
    "# Preprocess\n",
    "tokenized_input = vectorizer([sample_input])\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(tokenized_input)\n",
    "\n",
    "# Interpret prediction\n",
    "predicted_labels = (prediction > 0.5).astype(int)\n",
    "print(\"Prediction:\", predicted_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "sample_input =\"i love you\"\n",
    "\n",
    "# Preprocess\n",
    "tokenized_input = vectorizer([sample_input])\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(tokenized_input)\n",
    "\n",
    "# Interpret prediction\n",
    "predicted_labels = (prediction > 0.5).astype(int)\n",
    "print(\"Prediction:\", predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n"
     ]
    }
   ],
   "source": [
    "class_names = ['class1 : toxic', 'class2 : severe-toxic', 'class3 : obscene', 'class4 : threat', 'class5 : insult', 'class6 : identity hate']\n",
    "\n",
    "def score_comment(comment):\n",
    "    # Preprocess the input text using the vectorizer\n",
    "    tokenized_input = vectorizer([comment])  # vectorizer expects a list of strings\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(tokenized_input)\n",
    "    \n",
    "    # Assuming it's a multi-label classification with sigmoid activation\n",
    "    predicted_labels = (prediction > 0.5).astype(int)[0]\n",
    "    \n",
    "    # Map indices to class names\n",
    "    predicted_classes = [class_names[i] for i, val in enumerate(predicted_labels) if val == 1]\n",
    "    \n",
    "    # Create a dictionary for Gradio\n",
    "    prediction_dict = {cls: 1 for cls in predicted_classes}\n",
    "    \n",
    "    return prediction_dict\n",
    "\n",
    "# Create the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=score_comment, \n",
    "    inputs=gr.Textbox(lines=2, placeholder='Comment to score'),\n",
    "    outputs=gr.Label(num_top_classes=6)  # Adjust based on the number of classes\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
