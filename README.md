# Comment Toxicity Detection


Welcome to the **Comment Toxicity Detection** project! This repository contains tools and resources for detecting toxic language in text comments using machine learning techniques.

## Project Overview
The goal of this project is to build a robust model capable of identifying toxic comments. The application is designed to foster positive online interactions by filtering out harmful language. The model has been trained and evaluated using datasets from the [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).

## üîß Features
- **Custom-Trained Model**: The model was trained from scratch using the provided dataset and saved for later use.
- **Interactive Interface**: Uses [Gradio](https://gradio.app/) to provide an easy-to-use web interface for testing the model.
- **Multi-label Classification**: Detects various categories of toxicity such as:
  - Toxic
  - Severe Toxic
  - Obscene
  - Threat
  - Insult
  - Identity Hate

## Repository Contents
- **`Toxicity2.ipynb`**: Jupyter Notebook containing the model development and training process.
- **`toxicity.h5`**: Trained model weights.
- **`jigsaw-toxic-comment-classification-challenge/`**: Dataset folder.
- **`.gradio/`**: Configuration files for the Gradio interface.
- **`.gitignore`**: Specifies intentionally untracked files to ignore.

## üíΩ Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/ArianDilfanian/CommentToxicity.git
   cd CommentToxicity
   ```
2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## Usage
1. Launch the Gradio interface:
   ```bash
   python app.py
   ```
2. Open the displayed URL in your browser to test the model.
3. Input a comment to check for toxicity categories.

## üíø Dataset
The dataset used in this project is publicly available at Kaggle: [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).

## üèãÔ∏è‚Äç‚ôÇÔ∏è Model Training
The model was trained from scratch using the dataset provided by the challenge. Detailed training steps, including preprocessing, training, and evaluation, can be found in the `Toxicity2.ipynb` file.

## üí™ Contributing
Contributions are welcome! Feel free to submit a pull request or open an issue for suggestions or improvements.


## Acknowledgments
- Kaggle for providing the dataset.
- Gradio for the interactive interface.


---

Feel free to reach out if you have any questions or feedback about this project!
![Screenshot 2025-01-10 185056](https://github.com/user-attachments/assets/731d9682-3820-451b-bdfc-32e68a572f9a)

## üé• Video




https://github.com/user-attachments/assets/af6606da-e12e-4439-be49-cb768909b889






